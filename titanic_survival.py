# -*- coding: utf-8 -*-
"""Titanic Survival

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13X1nW8sN4yco5zE3ShIfxlIuWWF2L_Xy
"""

import numpy as np
import pandas as pd

df = pd.read_csv('tested.csv')

df

df.describe()

df.info()

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
le = LabelEncoder()
df['Sex'] = le.fit_transform(df['Sex'])

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
le = LabelEncoder()
df['Sex'] = le.fit_transform(df['Sex'])

# Assuming 'Embarked' is the categorical feature you want to one-hot encode
ohe = OneHotEncoder(sparse_output=False) # sparse=False to get a dense array directly
encoded_data = ohe.fit_transform(df[['Embarked']])

encoded_df = pd.DataFrame(encoded_data, columns=ohe.get_feature_names_out(['Embarked'])) # Use get_feature_names_out for column names
df = pd.concat([df, encoded_df], axis=1)
df = df.drop(['Embarked'], axis=1)
df

df = df.drop(['Name', 'Ticket', 'PassengerId', 'Cabin'], axis=1)

df.isnull().sum()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12, 7))
sns.heatmap(df.corr(), annot=True, cmap='viridis')
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x='Sex', y='Survived', data=df)
plt.title('Scatter Plot of Survived vs Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.ylabel('Survived')
plt.show()

survived_by_sex = df.groupby('Sex')['Survived'].sum()
survived_by_sex

# Plotting the distribution of Age
plt.figure(figsize=(8, 6))
sns.histplot(df['Age'], kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Plotting the distribution of Fare
plt.figure(figsize=(8, 6))
sns.histplot(df['Fare'], kde=True)
plt.title('Distribution of Fare')
plt.xlabel('Fare')
plt.ylabel('Frequency')
plt.show()

# Boxplot of Fare by Pclass
plt.figure(figsize=(8, 6))
sns.boxplot(x='Pclass', y='Fare', data=df)
plt.title('Boxplot of Fare by Pclass')
plt.xlabel('Pclass')
plt.ylabel('Fare')
plt.show()

# Countplot of Pclass
plt.figure(figsize=(8, 6))
sns.countplot(x='Pclass', hue='Survived', data=df)
plt.title('Countplot of Pclass by Survival')
plt.xlabel('Pclass')
plt.ylabel('Count')
plt.show()

# Countplot of SibSp
plt.figure(figsize=(8, 6))
sns.countplot(x='SibSp', hue='Survived', data=df)
plt.title('Countplot of SibSp by Survival')
plt.xlabel('SibSp')
plt.ylabel('Count')
plt.show()

# Countplot of Parch
plt.figure(figsize=(8, 6))
sns.countplot(x='Parch', hue='Survived', data=df)
plt.title('Countplot of Parch by Survival')
plt.xlabel('Parch')
plt.ylabel('Count')
plt.show()

# Plotting the distribution of Age for survived vs not survived
plt.figure(figsize=(8, 6))
sns.kdeplot(df[df['Survived'] == 0]['Age'], label='Not Survived')
sns.kdeplot(df[df['Survived'] == 1]['Age'], label='Survived')
plt.title('Distribution of Age for Survived vs Not Survived')
plt.xlabel('Age')
plt.ylabel('Density')
plt.legend()
plt.show()

# Plotting the distribution of Fare for survived vs not survived
plt.figure(figsize=(8, 6))
sns.kdeplot(df[df['Survived'] == 0]['Fare'], label='Not Survived')
sns.kdeplot(df[df['Survived'] == 1]['Fare'], label='Survived')
plt.title('Distribution of Fare for Survived vs Not Survived')
plt.xlabel('Fare')
plt.ylabel('Density')
plt.legend()
plt.show()

# Iterate through all columns except 'Survived'
for col in df.columns:
    if col != 'Survived':
        plt.figure(figsize=(8, 6))
        sns.scatterplot(x=col, y='Survived', data=df)
        plt.title(f'Scatter Plot of Survived vs {col}')
        plt.xlabel(col)
        plt.ylabel('Survived')
        plt.show()

df.isnull().sum()

from sklearn.impute import SimpleImputer

# Create an imputer object with the desired strategy (e.g., mean, median, most_frequent)
imputer = SimpleImputer(strategy='median')

# Fit the imputer to the numerical columns with missing values
numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

# Check if there are still any missing values
print(df.isnull().sum())

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Fit and transform the numerical features
numerical_cols = ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch', 'Sex', 'Embarked_C', 'Embarked_Q', 'Embarked_S']
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Display the normalized DataFrame
df.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define features (X) and target (y)
X = df.drop('Survived', axis=1)
y = df['Survived']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression(max_iter=1000) # Increased max_iter to ensure convergence
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

print(classification_report(y_test, y_pred))

conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
conf_matrix

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier


# Initialize and train other classification models
# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
print(f"Random Forest Accuracy: {rf_accuracy}")
print(classification_report(y_test, rf_pred))

# Gradient Boosting
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print(f"Gradient Boosting Accuracy: {gb_accuracy}")
print(classification_report(y_test, gb_pred))


# Support Vector Machine
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
svm_pred = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_pred)
print(f"SVM Accuracy: {svm_accuracy}")
print(classification_report(y_test, svm_pred))

# K-Nearest Neighbors
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
knn_pred = knn_model.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_pred)
print(f"KNN Accuracy: {knn_accuracy}")
print(classification_report(y_test, knn_pred))

#Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_accuracy}")
print(classification_report(y_test, dt_pred))